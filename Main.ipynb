{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "395         396        324          110                  3  3.5   3.5  9.04   \n",
       "396         397        325          107                  3  3.0   3.5  9.11   \n",
       "397         398        330          116                  4  5.0   4.5  9.45   \n",
       "398         399        312          103                  3  3.5   4.0  8.78   \n",
       "399         400        333          117                  4  5.0   4.0  9.66   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "395         1              0.82  \n",
       "396         1              0.84  \n",
       "397         1              0.91  \n",
       "398         0              0.67  \n",
       "399         1              0.95  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Serial No.'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "395        324          110                  3  3.5   3.5  9.04         1   \n",
       "396        325          107                  3  3.0   3.5  9.11         1   \n",
       "397        330          116                  4  5.0   4.5  9.45         1   \n",
       "398        312          103                  3  3.5   4.0  8.78         0   \n",
       "399        333          117                  4  5.0   4.0  9.66         1   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "395              0.82  \n",
       "396              0.84  \n",
       "397              0.91  \n",
       "398              0.67  \n",
       "399              0.95  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "Y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "395    0.82\n",
       "396    0.84\n",
       "397    0.91\n",
       "398    0.67\n",
       "399    0.95\n",
       "Name: Chance of Admit , Length: 400, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size= 0.2, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>301</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>334</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>305</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>307</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>318</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "93         301           97                  2  3.0   3.0  7.88         1\n",
       "23         334          119                  5  5.0   4.5  9.70         1\n",
       "299        305          112                  3  3.0   3.5  8.65         0\n",
       "13         307          109                  3  4.0   3.0  8.00         1\n",
       "90         318          106                  2  4.0   4.0  7.92         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[320 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22      , 0.17857143, 0.25      , ..., 0.42857143, 0.25      ,\n",
       "        1.        ],\n",
       "       [0.88      , 0.96428571, 1.        , ..., 0.85714286, 0.91911765,\n",
       "        1.        ],\n",
       "       [0.3       , 0.71428571, 0.5       , ..., 0.57142857, 0.53308824,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.70220588,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.74632353,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.22058824,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44      ,  0.39285714,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.58088235,  0.        ],\n",
       "       [ 0.2       ,  0.28571429,  0.5       ,  0.25      ,  0.42857143,\n",
       "         0.53676471,  1.        ],\n",
       "       [ 0.68      ,  0.71428571,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.57720588,  1.        ],\n",
       "       [ 0.68      ,  0.53571429,  1.        ,  0.625     ,  0.71428571,\n",
       "         0.53676471,  1.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.75      ,  0.75      ,  1.        ,\n",
       "         0.70955882,  1.        ],\n",
       "       [ 0.36      ,  0.5       ,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.38235294,  0.        ],\n",
       "       [ 0.4       ,  0.5       ,  0.25      ,  0.625     ,  0.28571429,\n",
       "         0.41544118,  0.        ],\n",
       "       [ 0.2       ,  0.35714286,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.24632353,  0.        ],\n",
       "       [ 0.4       ,  0.25      ,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.03676471,  0.        ],\n",
       "       [ 1.        ,  0.71428571,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.90441176,  1.        ],\n",
       "       [ 0.62      ,  0.67857143,  0.75      ,  0.75      ,  0.71428571,\n",
       "         0.65073529,  1.        ],\n",
       "       [ 0.92      ,  0.96428571,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.88970588,  1.        ],\n",
       "       [ 0.82      ,  0.82142857,  1.        ,  0.875     ,  0.57142857,\n",
       "         0.79411765,  1.        ],\n",
       "       [ 0.5       ,  0.42857143,  0.5       ,  0.5       ,  0.28571429,\n",
       "         0.41544118,  0.        ],\n",
       "       [ 0.66      ,  0.42857143,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.45588235,  1.        ],\n",
       "       [ 0.44      ,  0.64285714,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.48897059,  0.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.74264706,  1.        ],\n",
       "       [ 0.8       ,  0.82142857,  1.        ,  0.875     ,  0.42857143,\n",
       "         0.78676471,  1.        ],\n",
       "       [ 0.12      ,  0.10714286,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.125     ,  1.        ],\n",
       "       [ 0.36      ,  0.60714286,  0.25      ,  0.5       ,  0.71428571,\n",
       "         0.45955882,  0.        ],\n",
       "       [ 0.4       ,  0.5       ,  0.75      ,  0.125     ,  0.28571429,\n",
       "         0.42647059,  0.        ],\n",
       "       [ 0.68      ,  0.82142857,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.57352941,  1.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.71691176,  1.        ],\n",
       "       [ 0.82      ,  0.89285714,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.81617647,  1.        ],\n",
       "       [ 0.58      ,  0.5       ,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.29411765,  1.        ],\n",
       "       [ 0.16      ,  0.21428571,  0.25      ,  0.75      ,  0.42857143,\n",
       "         0.30514706,  0.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.625     ,  0.85714286,\n",
       "         0.41911765,  1.        ],\n",
       "       [ 0.5       ,  0.64285714,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.46323529,  1.        ],\n",
       "       [ 0.88      ,  0.85714286,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.86029412,  1.        ],\n",
       "       [ 0.4       ,  0.35714286,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.30147059,  1.        ],\n",
       "       [ 1.        ,  1.        ,  0.75      ,  1.        ,  1.        ,\n",
       "         0.84558824,  1.        ],\n",
       "       [ 0.48      ,  0.39285714,  0.25      ,  0.25      ,  0.42857143,\n",
       "         0.37132353,  0.        ],\n",
       "       [ 0.64      ,  0.64285714,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.65073529,  0.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.64705882,  1.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.5       ,  0.5       ,  0.71428571,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.44      ,  0.46428571,  0.5       ,  0.25      ,  0.42857143,\n",
       "         0.30147059,  1.        ],\n",
       "       [ 0.2       ,  0.25      ,  0.        ,  0.5       ,  0.14285714,\n",
       "        -0.14705882,  1.        ],\n",
       "       [ 0.48      ,  0.5       ,  0.25      ,  0.75      ,  0.57142857,\n",
       "         0.38602941,  0.        ],\n",
       "       [ 0.8       ,  0.85714286,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.82720588,  1.        ],\n",
       "       [ 0.56      ,  0.60714286,  0.        ,  0.625     ,  0.57142857,\n",
       "         0.70588235,  0.        ],\n",
       "       [ 0.74      ,  0.75      ,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.71323529,  0.        ],\n",
       "       [ 0.16      ,  0.46428571,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.49264706,  0.        ],\n",
       "       [ 0.88      ,  0.89285714,  1.        ,  0.75      ,  0.85714286,\n",
       "         0.6875    ,  1.        ],\n",
       "       [ 0.28      ,  0.46428571,  0.25      ,  0.5       ,  0.42857143,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.9       ,  0.89285714,  1.        ,  1.        ,  1.        ,\n",
       "         0.96323529,  1.        ],\n",
       "       [ 0.58      ,  0.39285714,  0.75      ,  0.875     ,  0.57142857,\n",
       "         0.53676471,  0.        ],\n",
       "       [ 0.6       ,  0.57142857,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.45588235,  1.        ],\n",
       "       [ 0.7       ,  0.71428571,  0.75      ,  0.75      ,  0.71428571,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.3       ,  0.46428571,  0.25      ,  0.5       ,  0.71428571,\n",
       "         0.34191176,  0.        ],\n",
       "       [ 0.36      ,  0.39285714,  0.25      ,  0.5       ,  0.57142857,\n",
       "         0.47426471,  0.        ],\n",
       "       [ 0.7       ,  0.71428571,  0.75      ,  0.625     ,  0.57142857,\n",
       "         0.63235294,  0.        ],\n",
       "       [ 0.12      ,  0.25      ,  0.25      ,  0.375     ,  0.28571429,\n",
       "         0.30514706,  0.        ],\n",
       "       [ 0.68      ,  0.67857143,  0.75      ,  0.5       ,  0.42857143,\n",
       "         0.66544118,  1.        ],\n",
       "       [ 0.56      ,  0.64285714,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.58823529,  0.        ],\n",
       "       [ 0.4       ,  0.53571429,  0.5       ,  0.625     ,  0.57142857,\n",
       "         0.54044118,  0.        ],\n",
       "       [ 0.92      ,  0.92857143,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.73161765,  1.        ],\n",
       "       [ 0.08      ,  0.10714286,  0.        ,  0.125     ,  0.        ,\n",
       "         0.16176471,  0.        ],\n",
       "       [ 0.        ,  0.42857143,  0.75      ,  0.25      ,  0.28571429,\n",
       "         0.09558824,  0.        ],\n",
       "       [ 0.72      ,  0.35714286,  0.75      ,  1.        ,  1.        ,\n",
       "         0.57352941,  1.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.5       ,  0.75      ,  1.        ,\n",
       "         0.52941176,  1.        ],\n",
       "       [ 0.96      ,  0.89285714,  0.75      ,  0.625     ,  0.85714286,\n",
       "         0.83088235,  1.        ],\n",
       "       [ 0.52      ,  0.53571429,  0.25      ,  0.625     ,  0.57142857,\n",
       "         0.52941176,  1.        ],\n",
       "       [ 0.8       ,  0.75      ,  1.        ,  1.        ,  0.71428571,\n",
       "         0.77573529,  1.        ],\n",
       "       [ 0.52      ,  0.46428571,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.5625    ,  0.        ],\n",
       "       [ 0.14      ,  0.21428571,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.17279412,  0.        ],\n",
       "       [ 0.68      ,  0.75      ,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.58455882,  0.        ],\n",
       "       [ 0.78      ,  0.67857143,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.72794118,  1.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.56617647,  1.        ],\n",
       "       [ 0.32      ,  0.46428571,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.375     ,  1.        ],\n",
       "       [ 0.42      ,  0.53571429,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.75      ,  0.5       ,  0.57142857,\n",
       "         0.65073529,  1.        ],\n",
       "       [ 0.28      ,  0.28571429,  0.75      ,  0.125     ,  0.28571429,\n",
       "         0.23529412,  0.        ],\n",
       "       [ 0.56      ,  0.53571429,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.39338235,  1.        ],\n",
       "       [ 0.34      ,  0.46428571,  0.25      ,  0.25      ,  0.57142857,\n",
       "         0.33088235,  0.        ],\n",
       "       [ 0.74      ,  0.67857143,  0.75      ,  0.75      ,  0.85714286,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.18      ,  0.07142857,  0.        ,  0.        , -0.14285714,\n",
       "         0.05147059,  0.        ],\n",
       "       [ 0.52      ,  0.46428571,  0.25      ,  0.375     ,  0.28571429,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.2       ,  0.42857143,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.35294118,  0.        ],\n",
       "       [ 0.38      ,  0.46428571,  1.        ,  0.625     ,  0.57142857,\n",
       "         0.5       ,  0.        ],\n",
       "       [ 0.3       ,  0.46428571,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.37867647,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(7, activation= 'relu', input_dim = 7))\n",
    "model.add(Dense(7, activation= 'relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer= 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 0.5129 - val_loss: 0.5314\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4703 - val_loss: 0.4845\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4288 - val_loss: 0.4379\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3859 - val_loss: 0.3910\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3420 - val_loss: 0.3432\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2972 - val_loss: 0.2938\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2509 - val_loss: 0.2443\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2046 - val_loss: 0.1930\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1563 - val_loss: 0.1404\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1097 - val_loss: 0.0932\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0703 - val_loss: 0.0567\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0417 - val_loss: 0.0319\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0236 - val_loss: 0.0176\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.0115\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0094\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0089\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0086\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0078 - val_loss: 0.0083\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0077 - val_loss: 0.0081\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0076 - val_loss: 0.0080\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0075 - val_loss: 0.0079\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0076\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0062\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0059\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0058\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0056\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0047\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, epochs= 100, validation_split= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70501965],\n",
       "       [0.54757446],\n",
       "       [0.79198605],\n",
       "       [0.751886  ],\n",
       "       [0.84041834],\n",
       "       [0.6305992 ],\n",
       "       [0.64068246],\n",
       "       [0.5085647 ],\n",
       "       [0.5059448 ],\n",
       "       [0.95416635],\n",
       "       [0.7978107 ],\n",
       "       [0.95328736],\n",
       "       [0.86736304],\n",
       "       [0.6228999 ],\n",
       "       [0.7561695 ],\n",
       "       [0.7082535 ],\n",
       "       [0.79350173],\n",
       "       [0.8509747 ],\n",
       "       [0.48627633],\n",
       "       [0.72398955],\n",
       "       [0.5698882 ],\n",
       "       [0.8020152 ],\n",
       "       [0.83410436],\n",
       "       [0.93925977],\n",
       "       [0.70636415],\n",
       "       [0.55768216],\n",
       "       [0.7268989 ],\n",
       "       [0.7462923 ],\n",
       "       [0.89201057],\n",
       "       [0.65242934],\n",
       "       [1.0100383 ],\n",
       "       [0.616475  ],\n",
       "       [0.7763204 ],\n",
       "       [0.77415806],\n",
       "       [0.7198289 ],\n",
       "       [0.5831641 ],\n",
       "       [0.5188408 ],\n",
       "       [0.7059766 ],\n",
       "       [0.9383056 ],\n",
       "       [0.77544016],\n",
       "       [0.9051919 ],\n",
       "       [0.6574517 ],\n",
       "       [0.9028259 ],\n",
       "       [0.6354937 ],\n",
       "       [0.97211826],\n",
       "       [0.69530755],\n",
       "       [0.7458204 ],\n",
       "       [0.82094854],\n",
       "       [0.6805155 ],\n",
       "       [0.6470416 ],\n",
       "       [0.7894679 ],\n",
       "       [0.50994515],\n",
       "       [0.74510366],\n",
       "       [0.736197  ],\n",
       "       [0.7036757 ],\n",
       "       [0.90589386],\n",
       "       [0.4088133 ],\n",
       "       [0.47927904],\n",
       "       [0.81286776],\n",
       "       [0.8214613 ],\n",
       "       [0.91203994],\n",
       "       [0.751377  ],\n",
       "       [0.8804485 ],\n",
       "       [0.69009876],\n",
       "       [0.51461613],\n",
       "       [0.8269432 ],\n",
       "       [0.86989754],\n",
       "       [0.7107548 ],\n",
       "       [0.62063277],\n",
       "       [0.7803728 ],\n",
       "       [0.7465105 ],\n",
       "       [0.4725458 ],\n",
       "       [0.68750095],\n",
       "       [0.6390085 ],\n",
       "       [0.8328964 ],\n",
       "       [0.37326407],\n",
       "       [0.64897144],\n",
       "       [0.5833385 ],\n",
       "       [0.6388787 ],\n",
       "       [0.5848594 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7698736398218013"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18f9f3e3188>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5900a330338e263bca32c7fc11f3f0fc42723957945f1be7e15053d3361e64c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
